{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Gradients in PyTorch using Autograd Package"]},{"cell_type":"markdown","metadata":{},"source":["## Pipeline\n","\n","1) Design model (input, output size, forward pass)\n","\n","2) Construct loss and optimizer\n","\n","3) Training loop\n","\n","    - forward pass: compute prediction\n","    - backward pass: gradients\n","    - update weights"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Prediction before training: f(5) = -0.492\n","epoch 1: w = 0.124, loss = 31.31781769\n","epoch 101: w = 1.738, loss = 0.09934704\n","epoch 201: w = 1.806, loss = 0.05454031\n","epoch 301: w = 1.856, loss = 0.02994200\n","epoch 401: w = 1.894, loss = 0.01643779\n","epoch 501: w = 1.921, loss = 0.00902416\n","epoch 601: w = 1.942, loss = 0.00495414\n","epoch 701: w = 1.957, loss = 0.00271976\n","epoch 801: w = 1.968, loss = 0.00149312\n","epoch 901: w = 1.976, loss = 0.00081970\n","epoch 1001: w = 1.982, loss = 0.00045001\n","epoch 1101: w = 1.987, loss = 0.00024705\n","epoch 1201: w = 1.990, loss = 0.00013563\n","epoch 1301: w = 1.993, loss = 0.00007446\n","epoch 1401: w = 1.995, loss = 0.00004088\n","epoch 1501: w = 1.996, loss = 0.00002244\n","epoch 1601: w = 1.997, loss = 0.00001232\n","epoch 1701: w = 1.998, loss = 0.00000676\n","epoch 1801: w = 1.998, loss = 0.00000371\n","epoch 1901: w = 1.999, loss = 0.00000204\n","epoch 2001: w = 1.999, loss = 0.00000112\n","epoch 2101: w = 1.999, loss = 0.00000061\n","epoch 2201: w = 2.000, loss = 0.00000034\n","epoch 2301: w = 2.000, loss = 0.00000019\n","epoch 2401: w = 2.000, loss = 0.00000010\n","epoch 2501: w = 2.000, loss = 0.00000006\n","epoch 2601: w = 2.000, loss = 0.00000003\n","epoch 2701: w = 2.000, loss = 0.00000002\n","epoch 2801: w = 2.000, loss = 0.00000001\n","epoch 2901: w = 2.000, loss = 0.00000001\n","epoch 3001: w = 2.000, loss = 0.00000000\n","epoch 3101: w = 2.000, loss = 0.00000000\n","epoch 3201: w = 2.000, loss = 0.00000000\n","epoch 3301: w = 2.000, loss = 0.00000000\n","epoch 3401: w = 2.000, loss = 0.00000000\n","epoch 3501: w = 2.000, loss = 0.00000000\n","epoch 3601: w = 2.000, loss = 0.00000000\n","epoch 3701: w = 2.000, loss = 0.00000000\n","epoch 3801: w = 2.000, loss = 0.00000000\n","epoch 3901: w = 2.000, loss = 0.00000000\n","epoch 4001: w = 2.000, loss = 0.00000000\n","epoch 4101: w = 2.000, loss = 0.00000000\n","epoch 4201: w = 2.000, loss = 0.00000000\n","epoch 4301: w = 2.000, loss = 0.00000000\n","epoch 4401: w = 2.000, loss = 0.00000000\n","epoch 4501: w = 2.000, loss = 0.00000000\n","epoch 4601: w = 2.000, loss = 0.00000000\n","epoch 4701: w = 2.000, loss = 0.00000000\n","epoch 4801: w = 2.000, loss = 0.00000000\n","epoch 4901: w = 2.000, loss = 0.00000000\n","epoch 5001: w = 2.000, loss = 0.00000000\n","epoch 5101: w = 2.000, loss = 0.00000000\n","epoch 5201: w = 2.000, loss = 0.00000000\n","epoch 5301: w = 2.000, loss = 0.00000000\n","epoch 5401: w = 2.000, loss = 0.00000000\n","epoch 5501: w = 2.000, loss = 0.00000000\n","epoch 5601: w = 2.000, loss = 0.00000000\n","epoch 5701: w = 2.000, loss = 0.00000000\n","epoch 5801: w = 2.000, loss = 0.00000000\n","epoch 5901: w = 2.000, loss = 0.00000000\n","epoch 6001: w = 2.000, loss = 0.00000000\n","epoch 6101: w = 2.000, loss = 0.00000000\n","epoch 6201: w = 2.000, loss = 0.00000000\n","epoch 6301: w = 2.000, loss = 0.00000000\n","epoch 6401: w = 2.000, loss = 0.00000000\n","epoch 6501: w = 2.000, loss = 0.00000000\n","epoch 6601: w = 2.000, loss = 0.00000000\n","epoch 6701: w = 2.000, loss = 0.00000000\n","epoch 6801: w = 2.000, loss = 0.00000000\n","epoch 6901: w = 2.000, loss = 0.00000000\n","epoch 7001: w = 2.000, loss = 0.00000000\n","epoch 7101: w = 2.000, loss = 0.00000000\n","epoch 7201: w = 2.000, loss = 0.00000000\n","epoch 7301: w = 2.000, loss = 0.00000000\n","epoch 7401: w = 2.000, loss = 0.00000000\n","epoch 7501: w = 2.000, loss = 0.00000000\n","epoch 7601: w = 2.000, loss = 0.00000000\n","epoch 7701: w = 2.000, loss = 0.00000000\n","epoch 7801: w = 2.000, loss = 0.00000000\n","epoch 7901: w = 2.000, loss = 0.00000000\n","epoch 8001: w = 2.000, loss = 0.00000000\n","epoch 8101: w = 2.000, loss = 0.00000000\n","epoch 8201: w = 2.000, loss = 0.00000000\n","epoch 8301: w = 2.000, loss = 0.00000000\n","epoch 8401: w = 2.000, loss = 0.00000000\n","epoch 8501: w = 2.000, loss = 0.00000000\n","epoch 8601: w = 2.000, loss = 0.00000000\n","epoch 8701: w = 2.000, loss = 0.00000000\n","epoch 8801: w = 2.000, loss = 0.00000000\n","epoch 8901: w = 2.000, loss = 0.00000000\n","epoch 9001: w = 2.000, loss = 0.00000000\n","epoch 9101: w = 2.000, loss = 0.00000000\n","epoch 9201: w = 2.000, loss = 0.00000000\n","epoch 9301: w = 2.000, loss = 0.00000000\n","epoch 9401: w = 2.000, loss = 0.00000000\n","epoch 9501: w = 2.000, loss = 0.00000000\n","epoch 9601: w = 2.000, loss = 0.00000000\n","epoch 9701: w = 2.000, loss = 0.00000000\n","epoch 9801: w = 2.000, loss = 0.00000000\n","epoch 9901: w = 2.000, loss = 0.00000000\n"]}],"source":["from pickletools import optimize\n","\n","\n","X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n","Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n","\n","X_test = torch.tensor([5], dtype = torch.float32)\n","\n","n_samples, n_features = X.shape\n","\n","input_size = n_features\n","output_size = n_features\n","\n","# w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n","\n","# # model prediction\n","# def forward(x):\n","#     return w * x\n","\n","# model = nn.Linear(input_size, output_size)\n","\n","\n","class LinearRegression(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(LinearRegression, self).__init__()\n","        # define layers\n","        self.lin = nn.Linear(input_dim, output_dim)\n","    \n","    def forward(self, x):\n","        return self.lin(x)\n","\n","model = LinearRegression(input_size, output_size)\n","\n","\n","print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n","\n","# Training\n","learning_rate = 0.01\n","n_iters = 10000\n","\n","loss = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n","\n","for epoch in range(n_iters):\n","    # prediction = forward pass\n","    y_pred = model(X)\n","    \n","    # loss\n","    l = loss(Y, y_pred)\n","\n","    # gradients = backward pass\n","    l.backward() # dl/dw\n","\n","    # update weights\n","    optimizer.step() # optimization step\n","\n","    # zero gradients\n","    optimizer.zero_grad()\n","\n","    if epoch % 100 == 0:\n","        [w, b] = model.parameters()\n","        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":2}
