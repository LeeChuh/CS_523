{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Autograd package in PyTorch"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import torch "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor y is: tensor([3.6201, 2.2130, 1.5203], grad_fn=<AddBackward0>)\n","Tensor z is: tensor([26.2109,  9.7951,  4.6228], grad_fn=<MulBackward0>)\n","Tensor z is: tensor(13.5430, grad_fn=<MeanBackward0>)\n"]}],"source":["# x requires the gradient\n","x = torch.randn(3, requires_grad=True)\n","y = x + 2\n","print(\"Tensor y is:\", y)\n","z = y*y*2\n","print(\"Tensor z is:\", z)\n","z = z.mean()\n","print(\"Tensor z is:\", z)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([4.8269, 2.9507, 2.0271])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["z.backward() # dz/dx\n","x.grad"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 0.7965, -0.1738, -0.3446], requires_grad=True)\n"]}],"source":["a = torch.randn(3, requires_grad=True)\n","print(a)\n","b = a + 2\n","c = b * b * 2\n","\n","# It's a Jacobian matrix times a Tensor\n","a_ = torch.tensor([1,1,1], dtype = torch.float32)\n","c.backward(a_)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([-0.2978,  1.1208,  1.9581], requires_grad=True)\n","tensor([1.7022, 3.1208, 3.9581])\n"]}],"source":["x = torch.randn(3, requires_grad=True)\n","print(x)\n","\n","# change the requires_grad_ of x to False\n","# x.requires_grad_(False)\n","\n","# create a copy of x that doesn't require the gradient\n","# y = x.detach()\n","# print(y)\n","\n","# stop PyTorch the gradient issue \n","with torch.no_grad():\n","    y = x + 2\n","    print(y)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n"]}],"source":["# set the requires_grad to True\n","weights = torch.ones(4, requires_grad=True)\n","\n","for epoch in range(3):\n","\n","    model_output = (weights * 3).sum()\n","    \n","    # calculate gradient\n","    model_output.backward()\n","    print(weights.grad)\n","\n","    # clear the gradients before doing the next iteration\n","    weights.grad.zero_()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":2}
